在训练模型中，很可能希望一个网络的不同模块具有不同的学习率，比如backbone的学习率为0.0001，而全连接层的学习率为0.01。为实现该功能，需要了解pytorch[优化器](https://pytorch.org/docs/stable/optim.html#base-class)的实现

## torch.optim.Optimizer基类
该类为Adam、SGD等其他优化器的父类，其他优化器通过继承该类进行实现。该类初始化有两个参数params合defaults  
params：可以存在两种形式
1. params为一个可迭带的对象，并且迭代元素为torch.Tensor，也就是待优化的张量，通常可以通过model.parameters()获得，在初始化函数内部，会将该输入转化为第二种dicts的输入形式
2. params为一个可迭代的对象，并且迭代的元素为dicts字典，下面是一个示例：
   ```ptyhon
   self.param_groups = [
       {'params': model.backbone.parameters(),'lr':0.0001},
       {'params': model.classifier.parameters(),'lr':0.0001}
   ]
   ```
   然后将defaults中存在的参数进行补充，比如defaults为：
   ```defaults = {'lr':0.01,'weight_decay':0.005}```
   替换后的params为：
   ```ptyhon
   self.param_groups = [
       {'params': model.backbone.parameters(),'lr':0.0001,'weight_decay':0.005},
       {'params': model.classifier.parameters(),'lr':0.01,'weight_decay':0.005}
   ]
   ```
3. 当optimizer调用step()函数进行梯度更新时,将对self.param_groups中每个组分别使用对应参数值进行更新