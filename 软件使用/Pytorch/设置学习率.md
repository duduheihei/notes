
## torch.optim.Optimizer基类
该类为Adam、SGD等其他优化器的父类，其他优化器通过继承该类进行实现。该类初始化有两个参数params合defaults  
params：可以存在两种形式
1. params为一个可迭带的对象，并且迭代元素为torch.Tensor，也就是待优化的张量，通常可以通过model.parameters()获得，在初始化函数内部，会将该输入转化为第二种dicts的输入形式
2. params为一个可迭代的对象，并且迭代的元素为dicts字典，下面是一个示例：
   ```ptyhon
   self.param_groups = [
       {'params': model.backbone.parameters(),'lr':0.0001},
       {'params': model.classifier.parameters(),'lr':0.0001}
   ]
   ```
   然后将defaults中存在的参数进行补充，比如defaults为：
   ```defaults = {'lr':0.01,'weight_decay':0.005}```
   替换后的params为：
   ```ptyhon
   self.param_groups = [
       {'params': model.backbone.parameters(),'lr':0.0001,'weight_decay':0.005},
       {'params': model.classifier.parameters(),'lr':0.01,'weight_decay':0.005}
   ]
   ```
3. 当optimizer调用step()函数进行梯度更新时,将对self.param_groups中每个组分别使用对应参数值进行更新

## 手动控制学习率随epoch或者iteration的变化，warmup
```python
def adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size):
    """Sets the learning rate
    # Adapted from PyTorch Imagenet example:
    # https://github.com/pytorch/examples/blob/master/imagenet/main.py
    """
    warmup_epoch = -1
    if epoch <= warmup_epoch:
        lr = 1e-6 + (initial_lr-1e-6) * iteration / (epoch_size * warmup_epoch)
    else:
        lr = initial_lr * (gamma ** (step_index))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr
```

### MultiStepLR以指定的steps将学习率以gamma系数进行衰减
每个group的lr分别更新，所以如果不同group的lr不同，不会受到影响
```python
from torch.optim.lr_scheduler import StepLR

def get_lr(self):
    if not self._get_lr_called_within_step:
        warnings.warn("To get the last learning rate computed by the scheduler, "
                        "please use `get_last_lr()`.", UserWarning)

    if self.last_epoch not in self.milestones:
        return [group['lr'] for group in self.optimizer.param_groups]
    return [group['lr'] * self.gamma ** self.milestones[self.last_epoch]
            for group in self.optimizer.param_groups]
```

## 给不同层设置不同的学习率
在训练模型中，很可能希望一个网络的不同模块具有不同的学习率，比如backbone的学习率为0.0001，而全连接层的学习率为0.01。为实现该功能，需要了解pytorch[优化器](https://pytorch.org/docs/stable/optim.html#base-class)的实现  

### 使用手动的方式（参考TSM代码）
1. 初始化optimizer的参数params，给不同层分配不同的学习率和系数
```python
    def get_optim_policies(self):
        first_conv_weight = []
        first_conv_bias = []
        normal_weight = []
        normal_bias = []
        lr5_weight = []
        lr10_bias = []
        bn = []
        custom_ops = []

        conv_cnt = 0
        bn_cnt = 0
        for m in self.modules():
            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.Conv3d):
                ps = list(m.parameters())
                conv_cnt += 1
                if conv_cnt == 1:
                    first_conv_weight.append(ps[0])
                    if len(ps) == 2:
                        first_conv_bias.append(ps[1])
                else:
                    normal_weight.append(ps[0])
                    if len(ps) == 2:
                        normal_bias.append(ps[1])
            elif isinstance(m, torch.nn.Linear):
                ps = list(m.parameters())
                if self.fc_lr5:
                    lr5_weight.append(ps[0])
                else:
                    normal_weight.append(ps[0])
                if len(ps) == 2:
                    if self.fc_lr5:
                        lr10_bias.append(ps[1])
                    else:
                        normal_bias.append(ps[1])

            elif isinstance(m, torch.nn.BatchNorm2d):
                bn_cnt += 1
                # later BN's are frozen
                if not self._enable_pbn or bn_cnt == 1:
                    bn.extend(list(m.parameters()))
            elif isinstance(m, torch.nn.BatchNorm3d):
                bn_cnt += 1
                # later BN's are frozen
                if not self._enable_pbn or bn_cnt == 1:
                    bn.extend(list(m.parameters()))
            elif len(m._modules) == 0:
                if len(list(m.parameters())) > 0:
                    raise ValueError("New atomic module type: {}. Need to give it a learning policy".format(type(m)))

        return [
            {'params': first_conv_weight, 'lr_mult': 5 if self.modality == 'Flow' else 1, 'decay_mult': 1,
             'name': "first_conv_weight"},
            {'params': first_conv_bias, 'lr_mult': 10 if self.modality == 'Flow' else 2, 'decay_mult': 0,
             'name': "first_conv_bias"},
            {'params': normal_weight, 'lr_mult': 1, 'decay_mult': 1,
             'name': "normal_weight"},
            {'params': normal_bias, 'lr_mult': 2, 'decay_mult': 0,
             'name': "normal_bias"},
            {'params': bn, 'lr_mult': 1, 'decay_mult': 0,
             'name': "BN scale/shift"},
            {'params': custom_ops, 'lr_mult': 1, 'decay_mult': 1,
             'name': "custom_ops"},
            # for fc
            {'params': lr5_weight, 'lr_mult': 5, 'decay_mult': 1,
             'name': "lr5_weight"},
            {'params': lr10_bias, 'lr_mult': 10, 'decay_mult': 0,
             'name': "lr10_bias"},
        ]
```
2. 手动依据epoch调整学习率和weight decay
```python
def adjust_learning_rate(optimizer, epoch, lr_type, lr_steps):
    """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
    if lr_type == 'step':
        decay = 0.1 ** (sum(epoch >= np.array(lr_steps)))
        lr = args.lr * decay
        decay = args.weight_decay
    elif lr_type == 'cos':
        import math
        lr = 0.5 * args.lr * (1 + math.cos(math.pi * epoch / args.epochs))
        decay = args.weight_decay
    else:
        raise NotImplementedError
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr * param_group['lr_mult']
        param_group['weight_decay'] = decay * param_group['decay_mult']
```

### 使用MultiStepLR，与手动类似，但是只需要在初始化optimizer时给不同的group分配不同的lr参数,该方法只能动态调整lr，无法对weight decay进行动态调整



