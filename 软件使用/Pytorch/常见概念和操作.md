## 叶子节点torch.Tensor.is_leaf()
pytorch Tensor提供了is_leaf方法用来确认该Tensor是否为叶子节点，那么pytorch中叶子节点的定义是什么？  
[官方解释](https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html)  
1. 所有requires_grad = False 的tensor一定是叶子节点
2. requires_grad = True的tensor，但是是由用户创建，而非其他操作计算得到的，也为叶子节点，对应的grad_fn=None
3. 在调用backward()时，只有叶子节点的grad会更新，如果想获得非叶子节点的梯度，需要调用retains_grad()方法

### pad
```python
import torch
import torch.nn.functional as F
a = torch.zeros((3,4,5))
#将a pad成5x5x5
pad = (0,0,0,1,1,1)
#前两个0表示最后一个维度无需pad，第三个数和第四个数表示倒数第二维度的末尾需要pad，第五个和第六个数表示倒数第三维度，首尾都需要增加一维
a = F.pad(a,pad,'constant',1)
print(a.shape)
```

## tensor
pytorch的tensor，用来存储array数值，具体的数值存储在torch.storage结构，可以使用tensor.storage()查看，stride对应矩阵每个维度的stide，可以用来判断矩阵是否contiguous。  
tensor还具有一些属性，包括grad,grad_fn,is_leaf,requires_grad,retains_grad等等
```
is_leaf：是否是叶子节点，叶子节点可以理解为反向传播的终止节点，requires_grad=False的tensor，或者requires_grad=True并且由用户创建的tensor
grad: tensor对应的梯度，在反向传播中会使用
grad_fn：反向传播时调用
requires_grad：顾名思义，是否需要计算梯度，如果手动将tensor的requires_grad设置为False，则梯度回传到此为止
retains_grad：在调用backward()时，只有叶子节点的grad会更新，如果想获得非叶子节点的梯度，需要调用retain_grad()方法,该方法会将非叶子节点的retains_grad值设置为True，叶子节点调用该函数不会做任何操作  

```

## contiguous
[参考知乎](https://zhuanlan.zhihu.com/p/64551412),这篇解读的非常棒
is_contiguous直观的解释是Tensor底层一维数组元素的存储顺序与Tensor按行优先一维展开的元素顺序是否一致。

## [Parameter](https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter)
继承自tensor，但是与tensor不同的是，该类通常存在于Module中当作层的可训练参数，并且当给Module的属性赋值时，会自动被M添加到Module的parameters列表属性中，可后续可以通过parameters()类方法进行遍历。而tensor则不会有此效果。比如RNN网络，想存储hidden_state以供下一次使用，但是很明显hidden_state不应该是模块的parameter

## [Module中buffer和parameter](https://zhuanlan.zhihu.com/p/89442276)
Module中可能存在一些参数是固定的，不需要进行梯度下降更新，比如transformer中的position embedding，因此需要有一个buffer list对该类参数进行存储。buffer是可以有梯度的，只是不会被.parameters()方法返回

## Module named_modules和named_children方法
named_modules采用的是深度优先遍历方法，会将每一层次的module都输出，因此父module和子module都会被返回。named_children只对第一层子module进行遍历
```python
model = ResNet(BasicBlock,[1,1,1,1],get_inplanes(),1,n_classes=16)
for name,module in model.named_modules():
    print(name)

```


## pytorch Module Hooks
[知乎介绍](https://zhuanlan.zhihu.com/p/276737137)  
[CSDN](https://blog.csdn.net/qq_45032341/article/details/105624136)  
pytorch为tensor和nn.module结构都提供了hook注册机制，能够在froward、backword前后插入一些操作，典型的应用包括：  
1. 通过打印模型信息调试程序
2. 对原模型进行包装，提取某些层的特征，得到一个输出特征的新模型
3. 梯度截断，只需要在backward后插入截断函数
4. pytorch一些复杂功能会用到，比如量化感知训练，分布式训练等等

1、torch.Tensor.register_hook(hook)  
2、torch.nn.Module.register_forward_hook  
3、torch.nn.Module.register_forward_pre_hook  
4、torch.nn.Module.register_backward_hook  

### Module register_forward_hook
在module调用forward后，再调用hook函数,注意，当注册的hook函数return具有返回值时，该module的输出会被修改，具体解释如下
```python
import torch
import torch.nn as nn

def hook_func(module,input,output):
    return output*(-1)

relu = nn.ReLU()
relu.register_forward_hook(hook_func)

x = torch.ones([5])
y = relu(x)
print(y) # tensor([-1., -1., -1., -1., -1.])
```

### RemovableHandle
在注册hook时，pytorch使用RemovableHandle对hooks支持hook的remove，RemovableHandle的定义如下：
```python
def register_forward_hook(self, hook: Callable[..., None]) -> RemovableHandle:
    r"""Registers a forward hook on the module.

    The hook will be called every time after :func:`forward` has computed an output.
    It should have the following signature::

        hook(module, input, output) -> None or modified output

    The input contains only the positional arguments given to the module.
    Keyword arguments won't be passed to the hooks and only to the ``forward``.
    The hook can modify the output. It can modify the input inplace but
    it will not have effect on forward since this is called after
    :func:`forward` is called.

    Returns:
        :class:`torch.utils.hooks.RemovableHandle`:
            a handle that can be used to remove the added hook by calling
            ``handle.remove()``
    """
    handle = hooks.RemovableHandle(self._forward_hooks)
    self._forward_hooks[handle.id] = hook
    return handle

import weakref #使用weakref模块，你可以创建到对象的弱引用，Python在对象的引用计数为0或只存在对象的弱引用时将回收这个对象。
class RemovableHandle(object):
    """A handle which provides the capability to remove a hook."""

    id: int
    next_id: int = 0

    def __init__(self, hooks_dict: Any) -> None:
        self.hooks_dict_ref = weakref.ref(hooks_dict)
        self.id = RemovableHandle.next_id
        RemovableHandle.next_id += 1

    def remove(self) -> None:
        hooks_dict = self.hooks_dict_ref()
        if hooks_dict is not None and self.id in hooks_dict:
            del hooks_dict[self.id]

    def __getstate__(self):
        return (self.hooks_dict_ref(), self.id)

    def __setstate__(self, state) -> None:
        if state[0] is None:
            # create a dead reference
            self.hooks_dict_ref = weakref.ref(OrderedDict())
        else:
            self.hooks_dict_ref = weakref.ref(state[0])
        self.id = state[1]
        RemovableHandle.next_id = max(RemovableHandle.next_id, self.id + 1)

    def __enter__(self) -> 'RemovableHandle':
        return self

    def __exit__(self, type: Any, value: Any, tb: Any) -> None:
        self.remove()
```

### Module _call_impl
hook具体的调用发生在Module类的_call_impl方法，该方法也是被重定义为__call__,该方法源码为：
```python
    def _call_impl(self, *input, **kwargs):
        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)
        # If we don't have any hooks, we want to skip the rest of the logic in
        # this function, and just call forward.
        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
                or _global_forward_hooks or _global_forward_pre_hooks):
            return forward_call(*input, **kwargs)
        # Do not call functions when jit is used
        full_backward_hooks, non_full_backward_hooks = [], []
        if self._backward_hooks or _global_backward_hooks:
            full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()
        if _global_forward_pre_hooks or self._forward_pre_hooks:
            for hook in (*_global_forward_pre_hooks.values(), *self._forward_pre_hooks.values()):
                result = hook(self, input)
                if result is not None:
                    if not isinstance(result, tuple):
                        result = (result,)
                    input = result

        bw_hook = None
        if full_backward_hooks:
            bw_hook = hooks.BackwardHook(self, full_backward_hooks)
            input = bw_hook.setup_input_hook(input)

        result = forward_call(*input, **kwargs)
        if _global_forward_hooks or self._forward_hooks:
            for hook in (*_global_forward_hooks.values(), *self._forward_hooks.values()):
                hook_result = hook(self, input, result)
                if hook_result is not None:
                    result = hook_result

        if bw_hook:
            result = bw_hook.setup_output_hook(result)

        # Handle the non-full backward hooks
        if non_full_backward_hooks:
            var = result
            while not isinstance(var, torch.Tensor):
                if isinstance(var, dict):
                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
                else:
                    var = var[0]
            grad_fn = var.grad_fn
            if grad_fn is not None:
                for hook in non_full_backward_hooks:
                    wrapper = functools.partial(hook, self)
                    functools.update_wrapper(wrapper, hook)
                    grad_fn.register_hook(wrapper)
                self._maybe_warn_non_full_backward_hook(input, result, grad_fn)

        return result

    __call__ : Callable[..., Any] = _call_impl
```

## Module _apply和apply
_apply供pytorch内部使用，主要用来支持网络的搬运，可以递归对parameters,buffers和parameters对应的grad进行批量搬运  
apply供外部调用，可以递归地实现对各个submodule的处理，递归使用的是深度优先

## torch.nn.Sequential
该类继承自Module，作用是将多个Module合并为一个Module，在自定义Moudle能够精简代码，如果初始化列表是多个Module，那么会调用add_module方法，将这些层依次添加到self._modules（Orderdict()结构）中，并且索引键值为从0开始的数值转为字符串，在forward时，对self._modules遍历可以保持顺序执行。初始化列表为Orderdict时，会使用原始的键值，而不是数字，源代码如下：
```python
class Sequential(Module):
    r"""A sequential container.
    Modules will be added to it in the order they are passed in the
    constructor. Alternatively, an ``OrderedDict`` of modules can be
    passed in. The ``forward()`` method of ``Sequential`` accepts any
    input and forwards it to the first module it contains. It then
    "chains" outputs to inputs sequentially for each subsequent module,
    finally returning the output of the last module.

    The value a ``Sequential`` provides over manually calling a sequence
    of modules is that it allows treating the whole container as a
    single module, such that performing a transformation on the
    ``Sequential`` applies to each of the modules it stores (which are
    each a registered submodule of the ``Sequential``).

    What's the difference between a ``Sequential`` and a
    :class:`torch.nn.ModuleList`? A ``ModuleList`` is exactly what it
    sounds like--a list for storing ``Module`` s! On the other hand,
    the layers in a ``Sequential`` are connected in a cascading way.

    Example::

        # Using Sequential to create a small model. When `model` is run,
        # input will first be passed to `Conv2d(1,20,5)`. The output of
        # `Conv2d(1,20,5)` will be used as the input to the first
        # `ReLU`; the output of the first `ReLU` will become the input
        # for `Conv2d(20,64,5)`. Finally, the output of
        # `Conv2d(20,64,5)` will be used as input to the second `ReLU`
        model = nn.Sequential(
                  nn.Conv2d(1,20,5),
                  nn.ReLU(),
                  nn.Conv2d(20,64,5),
                  nn.ReLU()
                )

        # Using Sequential with OrderedDict. This is functionally the
        # same as the above code
        model = nn.Sequential(OrderedDict([
                  ('conv1', nn.Conv2d(1,20,5)),
                  ('relu1', nn.ReLU()),
                  ('conv2', nn.Conv2d(20,64,5)),
                  ('relu2', nn.ReLU())
                ]))
    """

    _modules: Dict[str, Module]  # type: ignore[assignment]

    @overload
    def __init__(self, *args: Module) -> None:
        ...

    @overload
    def __init__(self, arg: 'OrderedDict[str, Module]') -> None:
        ...

    def __init__(self, *args):
        super(Sequential, self).__init__()
        if len(args) == 1 and isinstance(args[0], OrderedDict):
            for key, module in args[0].items():
                self.add_module(key, module)
        else:
            for idx, module in enumerate(args):
                self.add_module(str(idx), module)

    def _get_item_by_idx(self, iterator, idx) -> T:
        """Get the idx-th item of the iterator"""
        size = len(self)
        idx = operator.index(idx)
        if not -size <= idx < size:
            raise IndexError('index {} is out of range'.format(idx))
        idx %= size
        return next(islice(iterator, idx, None))

    @_copy_to_script_wrapper
    def __getitem__(self, idx) -> Union['Sequential', T]:
        if isinstance(idx, slice):
            return self.__class__(OrderedDict(list(self._modules.items())[idx]))
        else:
            return self._get_item_by_idx(self._modules.values(), idx)

    def __setitem__(self, idx: int, module: Module) -> None:
        key: str = self._get_item_by_idx(self._modules.keys(), idx)
        return setattr(self, key, module)

    def __delitem__(self, idx: Union[slice, int]) -> None:
        if isinstance(idx, slice):
            for key in list(self._modules.keys())[idx]:
                delattr(self, key)
        else:
            key = self._get_item_by_idx(self._modules.keys(), idx)
            delattr(self, key)

    @_copy_to_script_wrapper
    def __len__(self) -> int:
        return len(self._modules)

    @_copy_to_script_wrapper
    def __dir__(self):
        keys = super(Sequential, self).__dir__()
        keys = [key for key in keys if not key.isdigit()]
        return keys

    @_copy_to_script_wrapper
    def __iter__(self) -> Iterator[Module]:
        return iter(self._modules.values())

    # NB: We can't really type check this function as the type of input
    # may change dynamically (as is tested in
    # TestScript.test_sequential_intermediary_types).  Cannot annotate
    # with Any as TorchScript expects a more precise type
    def forward(self, input):
        for module in self:
            input = module(input)
        return input

```

### sub-pixel convolution和deconvolution,
subpixel convolution:是一种巧妙的图像及特征图upscale的方法，又叫pixel shuffle（像素洗牌）。对应pytorch nn.PixelShuffle模块，能够将$(c*r^{2},h,w)$大小的特征图按照反抽样的思路，重新排列为$(c,h*r,w*r)$大小，其中$r$为上采样因子
deconvolution:反卷积,通常也用于提高图像分辨率，其目的是模拟卷积的反操作，但实际上二者并不是真正意义上的反操作，仅仅是保持了输入和输出像素之间的关联性，即对于卷积操作来说，对输出有贡献的像素点，在反卷积时，会依据该关系，利用卷积后的feature，计算卷积前对应位置的值。具体的计算方式参考:[github](https://github.com/vdumoulin/conv_arithmetic)。注意：当stride>1时，不同的输入shape可能得到相同的输出shape，对于该情况，反卷积需要特定说明以消除歧义，pytorch 的ConvTranspose2d层通过设置output_padding 参数来显式消除该歧义。ConvTranspose2d的输入输出尺寸关系参考[官方文档](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html)

### dilated convolution空洞卷积
[参考](https://blog.csdn.net/gwplovekimi/article/details/90318426)
pytorch的卷积层有一个dilation参数，该参数大于1时，卷积就变为对应的空洞卷积，空洞卷积的目的是在保持参数量和计算量不变的情况下，增大卷积核的感受野。常用于图像分割和检测

### 梯度截断clip clamp
```python
loss.backward()
nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0, norm_type=2) #clip by norm
nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0) #clip by value
optimizer.step()
```

### deformable convolution可形变卷积
[知乎:可形变卷积](https://zhuanlan.zhihu.com/p/52476083)
可形变卷积的目的是，卷积核不再与固定距离的特征进行卷积计算，为卷积核每个位置增加一个x，y的偏移量，卷积核与x,y偏移后的特征进行计算，可以让网络获得更大的感受野，并且自适应地选取关键信息

## fold unfold
1.unfold是模拟滑窗操作，将输入大小为(N,C,H,W)的feature，依据kernel大小和stride，输出滑窗后的结果，比如kernel为(3,3),stride为1，fold后会输出(N,C*3*3,H*W)，然后该tensor再与kernel(C_out,C,3,3)相乘
2.fold比较有意思，很难找到确切的计算公式，输入输出的大小与unfold相反，官方文档说了这么句话：“This operation combines these local blocks into the large output tensor of shape (N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)(N,C,output_size[0],output_size[1],…) by summing the overlapping values.”,意思是重叠部分相加，我的理解是对于每个滑窗位置，会将对应的kernel位置的值，摆放到原位置，因此会存在重叠的情况，如果重叠，则会累加
3.猜想unfold用来计算卷积，fold用来计算反卷积







