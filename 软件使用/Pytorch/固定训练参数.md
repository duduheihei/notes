## 固定训练参数
如果网络中有部分模型无需计算梯度，那么可以用detach方法，该方法将 Variable 的grad_fn 设置为 None，因此BP不会再继续

## 训练时固定batch normalization层，需要重写model的train方法：
```python
def train(self, mode=True):
    super(ResNet, self).train(mode)
    self._freeze_stages()
    if mode and self.norm_eval:
        for m in self.modules():
            # trick: eval have effect on BatchNorm only
            if isinstance(m, _BatchNorm):
                m.eval()
```
