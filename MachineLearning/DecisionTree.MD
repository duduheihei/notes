## DecisionTree 决策树与随机森林
[博客园参考链接](https://www.cnblogs.com/fionacai/p/5894142.html)  

与logistics回归相比，决策树具有以下优缺点：  
优点：非线性；计算量小；可解释性  
缺点：准确性不足；过拟合；属性间关系不强  


## 随机森林  
尽管有剪枝等等方法，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的缺点。（可以理解成三个臭皮匠顶过诸葛亮）  

而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。Bagging策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。  

随机森林在bagging的基础上更进一步：  
1.  样本的随机：从样本集中用Bootstrap随机选取n个样本  
2.  特征的随机：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）  
3.  重复以上两步m次，即建立了m棵CART决策树  
4.  这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）  

### 关于调参：  
1.如何选取K，可以考虑有N个属性，取K=根号N  
               2.最大深度（不超过8层）  
               3.棵数  
               4.最小分裂样本树  
               5.类别比例

### 参数设置经验：  
min_samples_leaf 经验上必须大于100，如果一个节点都没有100个样本支持他的决策，一般都被认为是过拟合；max_depth 这个参数控制树的规模。决策树是一个非常直观的机器学习方法。一般我们都会把它的决策树结构打印出来观察，如果深度太深对于我们的理解是有难度的。


