[参考](https://blog.csdn.net/u012505617/article/details/108753869)
结论：最大似然函数等价交叉熵

## 交叉熵
![](pics/cross_entropy.gif)

## KL散度
![](pics/KLDiv.gif)

## 交叉熵与KL散度关系
![](pics/cross_entropy_and_KLDiv.gif)
当P的分布确定时，最小化交叉熵等价于最小化KL散度。因此pytorch分类时，选取两个loss没有实质性差别